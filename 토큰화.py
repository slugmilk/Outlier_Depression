# -*- coding: utf-8 -*-
"""토큰화.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bLEe1WsnPRYncev23sZUghx0T1jhE2bd
"""

pip install konlpy

import pandas as pd
from google.colab import files

file_uploaded = files.upload()

from pandas import DataFrame  as df

#데이터 불러오기 질문이랑 답변만
Data = df(columns=['question','answer'])


#파일이 여러 개 일때, 하나의 데이터 프레임으로 만들기 파일명이 파일명01.json,파일명02.json...일 때
num=['01','02','03','04','05','06','07','08','09','10']
for i in num:
    print(i)

    df = pd.read_excel('최종.xlsx')
    df = df[['question','answer']]
    Data=pd.concat([Data,df],ignore_index=True,axis = 0)

Data

pip install kss

import konlpy
from konlpy.tag import Okt
from tqdm.notebook import tqdm
import itertools
from gensim.models.ldamodel import LdaModel
from gensim.models.callbacks import CoherenceMetric
import gensim
from gensim import corpora, models
from gensim.models import CoherenceModel
from nltk.util import ngrams
#import kss

okt = Okt()

import numpy as np
import nltk

#데이터 프레임의 'question' 열의 값들을 str 형식으로 바꾸기
Data.question = Data.question.astype(str)


#데이터 프레임의 'question' 열의 값 중 keyword1이나 keyword 2가 포함된 행만 Data에 저장
#clean_Data = Data.loc[Data['question'].str.contains('keyword1|keyword2')]

#데이터 프레임의 'question' 열의 값 중 keyword1이나 keyword 2가 포함된 행은 삭제
#clean_Data = Data[~Data['question'].str.contains('keyword1|keyword2')]


#question와 answer 열을 기준으로 중복된 데이터를 삭제, inplace : 데이터 프레임을 변경할지 선택(원본을 대체)
Data.drop_duplicates(subset=['question','answer'], inplace=True)

#한글이 아니면 빈 문자열로 바꾸기
Data['question'] = Data['question'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣]',' ',regex=True)

#빈 문자열 NAN 값으로 바꾸기
Data = Data.replace({'': np.nan})
Data = Data.replace(r'^\s*$', None, regex=True)

#NAN 이 있는 행은 삭제
Data.dropna(how='any', inplace=True)



#인덱스 차곡차곡
Data = Data.reset_index (drop = True)

#데이터 프레임에 null 값이 있는지 확인
print(Data.isnull().values.any())

#텍스트 데이터를 리스트로 변환
Data_list=Data.question.values.tolist()

#리스트를 요소별로(트윗 하나) 가져와서 명사만 추출한 후 리스트로 저장
data_word=[]
for i in range(len(Data_list)):
    #try:
        data_word.append(okt.nouns(Data_list[i]))
    #except Exception as e:
       # print ('false')
        #continue

#트윗에서 명사만 추출해서 만든 리스트
data_word

import konlpy
import gensim
from konlpy.tag import Mecab
from gensim import corpora, models, similarities
from pprint import pprint
from gensim.models.coherencemodel import CoherenceModel
from gensim.models.wrappers import LdaMallet
import warnings
warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')

id2word=corpora.Dictionary(data_word)
texts = data_word
corpus=[id2word.doc2bow(text) for text in texts]

print(corpus[:1])

ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=3, id2word=id2word)
topics = ldamodel.print_topics(num_words=10)
for topic in topics:
  print(topic)

coherence_model_ldamodel = CoherenceModel(model=ldamodel, texts=texts, dictionary=id2word, coherence='c_v')
coherence_ldamodel = coherence_model_ldamodel.get_coherence()


def compute_coherence_values(dictionary, corpus, texts, limit, start=4, step=2):

    coherence_values = []
    model_list = []
    for num_topics in range(start, limit, step):
        model = gensim.models.ldamodel.LdaModel(corpus, num_topics=num_topics, id2word=id2word)
        model_list.append(model)
        coherencemodel = CoherenceModel(model=model, texts=data_word, dictionary=dictionary, coherence='c_v')
        coherence_values.append(coherencemodel.get_coherence())

    return model_list, coherence_values



# Can take a long time to run.
model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=texts, start=4, limit=21, step=2)




limit=21; start=4; step=2;
x = range(start, limit, step)
topic_num = 0
count = 0
max_coherence = 0
for m, cv in zip(x, coherence_values):
    print("Num Topics =", m, " has Coherence Value of", cv)
    coherence = cv
    if coherence >= max_coherence:
        max_coherence = coherence
        topic_num = m
        model_list_num = count
    count = count+1


# Select the model and print the topics
optimal_model = model_list[model_list_num]
model_topics = optimal_model.show_topics(formatted=False)
#print(optimal_model.print_topics(num_words=10))



def format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=texts):
    # Init output
    sent_topics_df = pd.DataFrame()

    # Get main topic in each document
    #ldamodel[corpus]: lda_model에 corpus를 넣어 각 토픽 당 확률을 알 수 있음
    for i, row in enumerate(ldamodel[corpus]):
        row = sorted(row, key=lambda x: (x[1]), reverse=True)
        # Get the Dominant topic, Perc Contribution and Keywords for each document
        for j, (topic_num, prop_topic) in enumerate(row):
            if j == 0:  # => dominant topic
                wp = ldamodel.show_topic(topic_num,topn=10)
                topic_keywords = ", ".join([word for word, prop in wp])
                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)
            else:
                break
    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']
    print(type(sent_topics_df))

    # Add original text to the end of the output
    #contents = pd.Series(texts)
    #sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)
    sent_topics_df = pd.concat([sent_topics_df, Data['question'],Data['answer']], axis=1)
    return(sent_topics_df)


df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=Data_list)

# Format
df_topic_tweet = df_topic_sents_keywords.reset_index()
df_topic_tweet.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'question','answer']

# Show각 문서에 대한 토픽
#df_dominant_topic=df_dominant_topic.sort_values(by=['Dominant_Topic'])
#df_topic_tweet



# Group top 5 sentences under each topic
sent_topics_sorteddf_mallet = pd.DataFrame()

sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')

for i, grp in sent_topics_outdf_grpd:
    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet,
                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)],
                                            axis=0)

# Reset Index
sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)


topic_counts = df_topic_sents_keywords['Dominant_Topic'].value_counts()
topic_counts.sort_index(inplace=True)

topic_contribution = round(topic_counts/topic_counts.sum(), 4)
topic_contribution

lda_inform = pd.concat([sent_topics_sorteddf_mallet, topic_counts, topic_contribution], axis=1)
lda_inform.columns=["Topic_Num", "Topic_Perc_Contrib", "Keywords", "question", "answer", "Num_Documents", "Perc_Documents"]
lda_inform = lda_inform[["Topic_Num","Keywords","Num_Documents","Perc_Documents"]]
lda_inform
#lda_inform.Topic_Num = lda_inform.Topic_Num.astype(int)
lda_inform['Topic_Num'] =lda_inform['Topic_Num'] +1
lda_inform.Topic_Num = lda_inform.Topic_Num.astype(str)
lda_inform['Topic_Num'] =lda_inform['Topic_Num'].str.split('.').str[0]
df_topic_tweet['Dominant_Topic'] =df_topic_tweet['Dominant_Topic'] +1
df_topic_tweet.Dominant_Topic = df_topic_tweet.Dominant_Topic.astype(str)
df_topic_tweet['Dominant_Topic'] =df_topic_tweet['Dominant_Topic'].str.split('.').str[0]

lda_inform.to_excel("tokenize.xlsx", index = None)
lda_inform

#토픽별 트윗 저장
for i in range(1,topic_num+1):
    globals()['df_{}'.format(i)]=df_topic_tweet.loc[df_topic_tweet.Dominant_Topic==str(i)]
    globals()['df_{}'.format(i)].sort_values('Topic_Perc_Contrib',ascending=False,inplace = True)
    globals()['df_{}'.format(i)].to_excel ("tokenize("+str(i)+")_tweet.xlsx", index = None)

df_1