# -*- coding: utf-8 -*-
"""섭식장애.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qatzETTU1CG7s51dlaoxEVCY6KCRwY39
"""

import pandas as pd
import numpy as np
import os
import sys
import urllib.request
from urllib.request import urlopen
from bs4 import BeautifulSoup
import datetime
import csv
import requests
from collections import Counter

client_id = "c3NcK94ofs3LM25Nu0t8"
client_secret = "OcnNINSc6N"
encText = urllib.parse.quote("섭식장애") #검색 키워드
url = "https://openapi.naver.com/v1/search/kin?query=" + encText +"&display=100" #json 결과

request = urllib.request.Request(url)
request.add_header("X-Naver-Client-Id",client_id)
request.add_header("X-Naver-Client-Secret",client_secret)
response = urllib.request.urlopen(request)
rescode = response.getcode()

if(rescode==200):
    response_body = response.read()
else:
    print("Error Code:" + rescode)

response_body=response_body.decode('utf-8').replace('\t', '').replace('\\', '')

response_body

response_body.split('{')[2].split('\n')

split = response_body.split('{')
links = []
for i in range(2, len(split)):
    links.append(split[i].split('\n')[2][8:-2])
df = pd.DataFrame(links)

df.columns = ['url']
df

# 결과를 저장할 리스트
titles = []
questions = []
answers = []
qa_urls = []
failed_urls = []
a_number = []

for i, url in enumerate(df.url):
    print("---------------------------",i,"---------------------------")
    response = requests.get(url)
    print(url)

    # candidate answer lists
    candid_answers = []

    if response.status_code == 200:
        html = response.text
        soup = BeautifulSoup(html, 'html.parser')

        # title and question
        try:
            title = soup.select_one("#content > div.question-content > div > div.c-heading._questionContentsArea.c-heading--default-old > div.c-heading__title > div.c-heading__title-inner > div.title").get_text().strip()
            question = soup.select_one("#content > div.question-content > div > div.c-heading._questionContentsArea.c-heading--default-old > div.c-heading__content").get_text().strip()
        except:
            try:
                title = soup.select_one("#content > div.question-content > div > div.c-heading._questionContentsArea.c-heading--multiple-old > div.c-heading__title > div.c-heading__title-inner > div.title").get_text().strip()
                question = soup.select_one("#content > div.question-content > div > div.c-heading._questionContentsArea.c-heading--multiple-old > div.c-heading__content").get_text().strip()
            except:
                try:
                    title = soup.select_one("#content > div.question-content > div > div.c-heading._questionContentsArea.c-heading--default > div.c-heading__title > div.c-heading__title-inner > div.title").get_text().strip()
                    question = '제목과 내용 동일'
                except:
                    try:
                        title = soup.select_one("#content > div.question-content > div > div.c-heading._questionContentsArea.c-heading--multiple > div.c-heading__title > div.c-heading__title-inner > div.title").get_text().strip()
                        question = '제목과 내용 동일'
                    except:
                        failed_urls.append(df['url'][i])
                        continue

        for j in range(1,15):
            try:
                temp = soup.select_one('#answer_' + str(j)).get_text().strip().split('\n\n\n\n\n\n')[2]
                temp = ' '.join(temp.split('\n\n')[:-1])
                temp = temp.replace('위 답변은 답변작성자가 경험과 지식을 바탕으로 작성한 내용입니다. 포인트로 감사할 때 참고해주세요.', ' ')
                temp = temp.replace('본 답변은 참고 용도로만 활용 가능하며 정확한 정보는 관련기관에서 확인해보시기 바랍니다.', ' ')
                temp = temp.replace('알아두세요', ' ')

                if temp != '':
                    titles.append(title)
                    questions.append(question)
                    answers.append(temp)
                    qa_urls.append(url)

            except:
                break
    else:
        print(response.status_code)

# Check saved data
print(len(titles))
print(len(questions))
print(len(answers))
print(len(qa_urls))
print(len(failed_urls))

# Convert List to DataFrame and Save
kin2 = pd.DataFrame(
                {'title' : titles,
                 'question' : questions,
                 'answer' : answers,
                 'url' : qa_urls
                })

kin2

from google.colab import drive
drive.mount('/content/drive')

kin2.to_excel('/content/drive/MyDrive/kin2excel.xlsx')