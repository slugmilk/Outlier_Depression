# -*- coding: utf-8 -*-
"""LDA

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1o_kury8wEIya5M-Sp3w9xx8Fh1-Rirjb
"""

import pandas as pd
import csv
from collections import Counter
from google.colab import files

file_uploaded = files.upload()

data = pd.read_excel("최종_유형나눔.xlsx")
data

Counter(data['disorder'])

pip install konlpy

pip install kss

from konlpy.tag import Okt
from tqdm.notebook import tqdm
import itertools
from gensim.models.ldamodel import LdaModel
from gensim.models.callbacks import CoherenceMetric
import gensim
from gensim import corpora, models
from gensim.models import CoherenceModel
from nltk.util import ngrams
import kss
import numpy as np

depression = data[data['disorder']=="우울장애"]['question']
pd.DataFrame(data=depression)

def get_nouns(tokenizer, sentence):
    tagged = tokenizer.pos(sentence)
    nouns = [s for s, t in tagged]
    print("[tokenized] : ", nouns)
    return nouns

def tokenize(sentences):
    tokenizer = Okt()
    processed_data = []
    for sent in tqdm(sentences):
        print("---"*20)
        print("[original] : ", sent)
        processed_data.append(get_nouns(tokenizer, sent))
    return processed_data

depression_tokenized = tokenize(depression)

def dictionary(data_word):
    # make a dictionary
    print("Make a dictionary...")
    id2word=corpora.Dictionary(data_word)
    id2word.filter_extremes(no_below = 20) #20회 이하로 등장한 단어는 삭제
    texts = data_word
    corpus=[id2word.doc2bow(text) for text in texts]
    return id2word, texts, corpus

def compute_coherence_values(dictionary, corpus, texts, start, limit, step):
    print("Compute coherence values...")
    coherence_values = []
    model_list = []
    print("[start] : ", start)
    print("[limit] : ", limit)
    print("[step] : ", step)
    for num_topics in range(start, limit, step):
        print(num_topics, "in progress")
        model = gensim.models.ldamodel.LdaModel(corpus, num_topics=num_topics, id2word=dictionary)
        model_list.append(model)
        coherencemodel = CoherenceModel(model=model, texts=data_word, dictionary=dictionary, coherence='c_v')
        coherence_values.append(coherencemodel.get_coherence())
    return model_list, coherence_values

def LDA(data_word, Data_list, start, limit, step):
    id2word, texts, corpus = dictionary(data_word)
    # create lda instance
    print("Create LDA instance...")
    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=20, id2word=id2word)

    # calculating coherence to find optimal k (the number of topics)
    coherence_model_ldamodel = CoherenceModel(model=ldamodel, texts=texts, dictionary=id2word, coherence='c_v')
    coherence_ldamodel = coherence_model_ldamodel.get_coherence()

    # Can take a long time to run.
    model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=texts, start=start, limit=limit, step=step)
    x = range(start, limit, step)
    topic_num = 0
    count = 0
    max_coherence = 0
    for m, cv in zip(x, coherence_values):
        print("Num Topics =", m, " has Coherence Value of", cv)
        coherence = cv
        if coherence >= max_coherence:
            max_coherence = coherence
            topic_num = m
            model_list_num = count
        count = count+1

    # Select the model and print the topics
    optimal_model = model_list[model_list_num]
    model_topics = optimal_model.show_topics(formatted=False, num_words=30)
    return optimal_model, model_topics

data_word = depression_tokenized   #tokenized data
Data_list = depression             #untokenized data
start=3; limit=5; step=1;          #the number of topics

##------check------##
print(Data_list[0])
print("\n", np.array(data_word[0]))

model, topics = LDA(data_word, Data_list, start, limit, step)

topics

topics.to_csv ("topics.csv", index = None)

"""N-gram 추출"""

def get_nouns(tokenizer, sentence):
    tagged = tokenizer.pos(sentence)
    nouns = [s for s, t in tagged]
#     nouns = [s for s, t in tagged if t in ['SL', 'NNG', 'NNP', 'VV', 'VA', 'XR'] and len(s) > 1]
    print("[tokenized] : ", nouns)
    return nouns

def make_ngram(series, ngram):
    res = []
    for i in tqdm(range(len(series))):
        res.append(["_".join(w) for w in ngrams(series[i], ngram)])
    return res

depression_t2 = tokenize(depression)

depression_bigram = make_ngram(depression_t2, 2)
depression_bigram[7]

depression_trigram = make_ngram(depression_t2, 3)
depression_trigram[7]

def ngram_frequency(ngram, filename):
    data = list(itertools.chain(*ngram))
    cnt = pd.Series(data).value_counts()
    cnt = cnt[cnt >= 5]
    cnt.to_csv("depression_"+filename+".csv")
    return cnt

cnt_trigram = ngram_frequency(depression_trigram, "trigram")
cnt_bigram = ngram_frequency(depression_bigram, "bigram")