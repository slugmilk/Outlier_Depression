# 우울증 원인 분석 및 환자 예측 프로젝트 🧠📊

이 프로젝트는 우울증의 원인을 분석하고 우울증 환자를 예측하는 모델을 개발하는 것을 목적으로 합니다. "HCIT 보건 2팀"에서 진행한 이 프로젝트는 우울증과 관련된 정신 건강 문제를 탐구하기 위해 다양한 데이터 소스를 활용했습니다.

## 📋 목차
1. [프로젝트 정의](#프로젝트-정의)
2. [데이터 수집](#데이터-수집)
3. [진행 과정](#진행-과정)
4. [전처리 과정](#전처리-과정)
5. [토픽 모델링 결과](#토픽-모델링-결과)
6. [결론](#결론)
7. [참고 자료](#참고-자료)

## 프로젝트 정의 📝

이 프로젝트의 주요 목표는 우울증의 원인을 식별하고 분석하며, 잠재적으로 우울증을 앓고 있는 환자들을 예측하는 모델을 개발하는 것입니다.

## 데이터 수집 📚

프로젝트를 위해 다양한 경로로 데이터를 수집하였습니다:
- **네이버 지식인 크롤링**: 우울증과 관련된 사용자 생성 콘텐츠를 추출하였습니다.
- **네이버 뉴스 크롤링**: 우울증 및 정신 건강 문제와 관련된 기사를 수집하였습니다.
- **유튜브 댓글 크롤링**: 정신 건강 주제와 관련된 사용자 댓글을 수집하였습니다.

또한, 다양한 정신 건강 상태와 관련된 키워드도 함께 분석하였습니다:
- **섭식장애**: 폭식증, 거식증, 프로아나.
- **불안장애**: 공황장애, 강박증, PTSD, 신경증.
- **우울장애**: 우울증, 우울감.

## 진행 과정 🛠️

수집된 데이터를 분석하기 위해 다음과 같은 과정을 거쳤습니다:

1. **토픽 모델링과 TF-IDF 사용**: 수집된 데이터의 주제와 특징을 파악하기 위해 토픽 모델링과 TF-IDF를 사용하였습니다. 
    - **토픽 모델링**: `LDA(Latent Dirichlet Allocation)` 알고리즘을 사용하여 주요 키워드를 추출하였습니다. 파이썬 코드를 통해 크롤링된 파일을 불러온 뒤, Konlpy 패키지를 활용하여 형태소 분석 엔진으로 명사를 추출하였습니다. LDA 모듈을 이용하여 일관성 지수를 계산하고, 값이 높은 텍스트를 토픽으로 반환하였습니다.
    - 결과는 CSV 파일로 저장하여 토픽이 어떻게 추출되었는지 확인할 수 있도록 하였습니다.

2. **데이터 전처리**: 텍스트 데이터의 품질을 높이기 위해 전처리 과정을 거쳤습니다.
    - **불필요한 기호 제거**: HTML 태그, 이모티콘, 특수문자 등을 제거하여 순수한 텍스트만 남기도록 처리하였습니다.
    - **불용어 제거**: 조사, 접속사, 불필요한 감탄사 등 분석에 의미가 없는 단어를 제거하여 텍스트의 핵심 내용을 부각시켰습니다.
    - **토큰화**: 텍스트를 단어 또는 형태소 단위로 분할하여 분석에 적합한 형태로 만들었습니다.
    - **키워드 추출 및 라벨링**: 전처리된 데이터에서 특정 키워드를 중심으로 분류하고, 각 데이터에 우울증, 불안장애, 섭식장애 등과 같은 라벨을 부여하였습니다.

3. **결과 분석**: LDA를 통해 도출된 토픽 및 키워드를 활용하여 분석을 진행하였고, 결과를 CSV 파일로 저장하여 추출된 토픽을 시각화하였습니다.

## 전처리 과정 🧹

수집된 데이터의 품질을 향상시키기 위해 다음과 같은 전처리 과정을 거쳤습니다:

1. **데이터 수집**: 네이버 지식인, 뉴스 기사, 유튜브 댓글 등 다양한 소스에서 우울증 관련 텍스트 데이터를 크롤링하였습니다.

2. **텍스트 전처리**:
    - **불필요한 기호 제거**: 수집된 데이터에서 HTML 태그, 이모티콘, 특수문자 등을 제거하여 분석에 적합한 순수한 텍스트만 남도록 처리하였습니다.
    - **불용어 제거**: 분석에 의미가 없는 단어(예: 조사, 접속사, 불필요한 감탄사 등)를 제거하여 텍스트의 핵심 내용을 부각시켰습니다.
    - **토큰화**: 텍스트를 단어 또는 형태소 단위로 분할하여 데이터 분석에 사용할 수 있도록 하였습니다.

3. **키워드 추출 및 라벨링**: 전처리된 데이터에서 특정 키워드를 중심으로 분류하고, 각 데이터에 우울증, 불안장애, 섭식장애 등과 같은 라벨을 부여하여 후속 분석이 가능하도록 하였습니다.

4. **데이터 정제**: 중복 데이터 제거, 누락된 정보 채우기 등의 과정을 통해 데이터의 일관성을 높였습니다.

## 토픽 모델링 결과 🔍

LDA를 통해 추출된 토픽 결과는 다음과 같습니다:

| Topic_Num | Keywords                                        | Num_Documents | Perc_Documents |
|-----------|-------------------------------------------------|---------------|----------------|
| 1         | 공황장애, 치료, 불안장애, 갑자기, 증상, 때, 것, 걱정, 손, 몸 | 22            | 0.0432         |
| 2         | 제, 것, 치료, 증상, 생각, 말, 걱정, 아이, 때, 계속   | 29            | 0.057          |
| 3         | 치료, 생각, 불안장애, 저, 것, 병원, 때, 걱정, 증상, 상담 | 27            | 0.053          |
| 4         | 치료, 강박증, 제, 증상, 것, 약, 때, 공황장애, 때문에, 병원 | 76            | 0.1493         |
| ...       | ...                                             | ...           | ...            |

이 표는 각 토픽에 포함된 주요 키워드와 해당 토픽에 속하는 문서의 수 및 문서 비율을 나타냅니다.

## 결론 🤔

이번 프로젝트는 팀원 모두에게 새로운 도전이었습니다. 우울증 및 관련 정신 건강 문제에 대한 데이터를 수집하고 분석하면서 다양한 시도를 해보았지만, 팀원들의 배경 지식과 인사이트 도출 경험이 부족한 점이 한계로 작용하였습니다. 이러한 경험을 통해 데이터 분석 및 모델링에 필요한 깊이 있는 배경 지식과 경험을 쌓는 것이 중요하다는 것을 깨달았습니다. 앞으로 이러한 경험을 발판으로 삼아 더욱 발전된 프로젝트를 진행하고자 합니다.

## 참고 자료 📚

- [ETRI AI 오픈랩 튜토리얼](https://nanum.etri.re.kr/devTutorialDetail?id=632)
- [네이버 뉴스 기사](https://n.news.naver.com/mnews/article/020/0003349076)
- 네이버 지식백과
- 국민건강보험공단
- [네이버 블로그](https://blog.naver.com/brainshh/222743320215)
- 그 외: 동아일보, 뉴스1, 아시아투데이, 뉴시스, 코메디닷컴
